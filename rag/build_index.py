# rag/build_index.py
from dotenv import load_dotenv
load_dotenv()

import os
from langchain.vectorstores import FAISS
from utils.embedding_loader import load_embedding_model
from langchain.docstore.document import Document
from langchain.text_splitter import CharacterTextSplitter

def load_markdown_files(root_dir):
    all_chunks = []
    splitter = CharacterTextSplitter(separator="\n\n", chunk_size=500, chunk_overlap=50)

    for foldername, _, filenames in os.walk(root_dir):
        for filename in filenames:
            if filename.endswith(".md"):
                filepath = os.path.join(foldername, filename)
                with open(filepath, 'r', encoding='utf-8') as f:
                    text = f.read()

                # åŠ å…¥å…ƒä¿¡æ¯ï¼Œæ–¹ä¾¿ä¹‹åå›æº¯å¼•ç”¨æ¥æº
                metadata = {
                    "source": os.path.relpath(filepath, root_dir)
                }
                chunks = splitter.split_text(text)
                for chunk in chunks:
                    all_chunks.append(Document(page_content=chunk, metadata=metadata))

    return all_chunks

def build_index(doc_dir="documents", index_dir="rag/joja_index"):
    print("ğŸ” æ­£åœ¨åŠ è½½å¹¶åˆ‡åˆ† Markdown æ–‡æ¡£...")
    docs = load_markdown_files(doc_dir)

    if not docs:
        raise ValueError(f"âŒ æœªæ‰¾åˆ°æœ‰æ•ˆæ–‡æ¡£ï¼Œè¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®: {doc_dir}")

    print(f"ğŸ“š å…±åˆ‡åˆ†å‡º {len(docs)} ä¸ªæ®µè½ï¼Œå¼€å§‹ç”Ÿæˆå‘é‡...")
    embeddings = load_embedding_model("openai")  # æˆ– "gemini"
    vectordb = FAISS.from_documents(docs, embeddings)

    vectordb.save_local(index_dir)
    print(f"âœ… å‘é‡ç´¢å¼•æ„å»ºå®Œæˆï¼Œä¿å­˜åœ¨ {index_dir}")

if __name__ == "__main__":
    build_index()
